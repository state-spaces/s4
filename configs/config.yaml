# @package _global_
defaults:
  - _self_
  - experiment: base # Specifies model and pipeline, equivalent to next two lines
  # - model: s4 # Model backbone
  # - pipeline: cifar # Specifies collection of configs, equivalent to next 5 lines
  # Pipelines should specify /loader, /dataset, /task, /encoder, /decoder (ideally in that order)
  # # - loader: torch # Dataloader (e.g. handles batches)
  # # - dataset: cifar # Defines the data (x and y pairs)
  # # - task: multiclass_classification # Defines loss and metrics
  # # - encoder: null # Interface between data and model
  # # - decoder: null # Interface between model and targets
  - callbacks:
      - base
      - checkpoint

# Additional arguments used to configure the training loop
# Most of these set combinations of options in the PL trainer, add callbacks, or add features to the optimizer
train:
  seed: 0
  # These three options are used by callbacks (checkpoint, monitor) and scheduler
  # Most of them are task dependent and are set by the pipeline
  interval: ??? # Should be specified by scheduler. Also used by LR monitor
  monitor: ??? # Should be specified by pipeline. Used by scheduler (plateau) and checkpointer
  mode: ??? # Should be specified by pipeline. Used by scheduler (plateau) and checkpointer
  ema: 0.0 # Moving average model for validation # TODO move into callback
  test: False # Test after training
  debug: False # Special settings to make debugging more convenient
  ignore_warnings: False # Disable python warnings
  # These control state
  state:
    mode: null # [ None | 'none' | 'reset' | 'bptt' | 'tbptt' ]
    chunk_len: null  # [ None | int ] chunk length for tbptt (used by TBPTTDataLoader)
    overlap_len: null # [ None | int ] overlap length for tbptt (used by TBPTTDataLoader)
    n_context: 0 # How many steps to use as memory context. Must be >= 0 or None (null), meaning infinite context
    n_context_eval: ${.n_context}
  # Convenience keys to allow grouping runs
  sweep: null
  group: null

  benchmark_step: False # Whether to benchmark the step function
  benchmark_step_k: 1 # Multipler for loader.batch_size when benchmarking step function with large batch sizes than the dataset
  benchmark_step_T: 1 # Number of additional repeats to benchmark the step function
  checkpoint_path: null # Path to checkpoint file: only used for visualization at the moment
  visualizer: 'filters' # Which visualizer to use: [ 'filters' | 'forecasting' ]
  disable_dataset: False # Disable dataset loading

# We primarily use wandb so this is moved to top level for convenience
# Set ~wandb or wandb=null or wandb.mode=disabled to disable logging
# If other loggers are added, it would make sense to put this one level lower under train/ or logger/
wandb:
  project: hippo
  group: ""
  job_type: training
  mode: online # choices=['online', 'offline', 'disabled']
  save_dir: null
  id: null # pass correct id to resume experiment!
  # Below options should not need to be specified
  # entity: ""  # set to name of your wandb team or just remove it
  # log_model: False
  # prefix: ""
  # job_type: "train"
  # tags: []
